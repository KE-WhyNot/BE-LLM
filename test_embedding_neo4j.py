#!/usr/bin/env python3
"""
ë§¤ì¼ê²½ì œ RSS + ì„ë² ë”© + Neo4j í†µí•© í…ŒìŠ¤íŠ¸
ì‹¤ì œ RSS ë°ì´í„°ë¡œ ì„ë² ë”© ìƒì„±í•˜ê³  Neo4jì— ì €ì¥í•˜ëŠ” í…ŒìŠ¤íŠ¸
"""

import asyncio
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.services.workflow_components.mk_rss_scraper import MKNewsScraper


async def test_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (RSS â†’ ì„ë² ë”© â†’ Neo4j)"""
    print("ğŸš€ ë§¤ì¼ê²½ì œ RSS + ì„ë² ë”© + Neo4j í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    scraper = MKNewsScraper()
    
    try:
        # 1. RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœê·¼ 3ì¼, ì ì€ ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸)
        print("ğŸ“° 1ë‹¨ê³„: RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘...")
        articles = await scraper.scrape_all_feeds(days_back=3)
        
        if not articles:
            print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì²˜ìŒ 5ê°œ ê¸°ì‚¬ë§Œ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©
        test_articles = articles[:5]
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ìš© {len(test_articles)}ê°œ ê¸°ì‚¬ë¡œ ì§„í–‰")
        
        # 2. ì„ë² ë”© ìƒì„±
        print("\nğŸ§  2ë‹¨ê³„: ì„ë² ë”© ìƒì„±...")
        articles_with_embeddings = scraper.generate_embeddings(test_articles)
        
        print(f"âœ… {len(articles_with_embeddings)}ê°œ ê¸°ì‚¬ì— ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
        # ì„ë² ë”© ì •ë³´ ì¶œë ¥
        for i, article in enumerate(articles_with_embeddings, 1):
            if article.embedding:
                print(f"   {i}. {article.title[:50]}... (ì„ë² ë”© ì°¨ì›: {len(article.embedding)})")
            else:
                print(f"   {i}. {article.title[:50]}... (ì„ë² ë”© ìƒì„± ì‹¤íŒ¨)")
        
        # 3. Neo4j ì €ì¥ (Neo4jê°€ ìˆì„ ë•Œë§Œ)
        print("\nğŸ”— 3ë‹¨ê³„: Neo4j ì €ì¥...")
        if scraper.neo4j_graph:
            storage_stats = await scraper.store_to_neo4j(articles_with_embeddings)
            print(f"âœ… Neo4j ì €ì¥ ì™„ë£Œ: {storage_stats}")
            
            # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\nğŸ” 4ë‹¨ê³„: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
            search_results = await scraper.search_similar_articles("ì‚¼ì„±ì „ì", limit=3)
            print(f"âœ… ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            
            for i, result in enumerate(search_results, 1):
                print(f"   {i}. {result['title'][:50]}... (ìœ ì‚¬ë„: {result['similarity']:.3f})")
        else:
            print("âš ï¸ Neo4j ì—°ê²°ì´ ì—†ì–´ ì €ì¥ ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            print("   Neo4jë¥¼ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•œ í›„ ë‹¤ì‹œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”.")
        
        print("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_full_pipeline())
    if success:
        print("\nâœ… ë§¤ì¼ê²½ì œ RSS â†’ ì„ë² ë”© â†’ Neo4j íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
        print("\nğŸ“ ì‚¬ìš© ë°©ë²•:")
        print("1. Neo4j ì„¤ì¹˜ ë° ì‹¤í–‰ (ì„ íƒì‚¬í•­)")
        print("2. ì§€ì‹ê·¸ë˜í”„ ì—…ë°ì´íŠ¸: await update_mk_knowledge_graph()")
        print("3. ë‰´ìŠ¤ ê²€ìƒ‰: await search_mk_news('ê²€ìƒ‰ì–´')")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

