"""
LLM ê´€ë¦¬ì (Gemini 2.0 Flash ì „ìš©)
ê¹”ë”í•˜ê²Œ Geminië§Œ ì‚¬ìš©í•˜ë„ë¡ ë‹¨ìˆœí™”
"""

from typing import Optional, List, Dict, Any
from langchain_google_genai import ChatGoogleGenerativeAI
from app.config import settings
from app.utils.common_utils import CacheManager
import hashlib
import time

try:
    import numpy as _np
    from sentence_transformers import SentenceTransformer
except Exception:
    _np = None
    SentenceTransformer = None


class LLMManager:
    """LLM ê´€ë¦¬ì (Gemini ì „ìš©)"""
    
    def __init__(self):
        self.llm_cache = {}
        self.default_model = "gemini-2.0-flash"  # ì •ì‹ 2.0 ë²„ì „, ë†’ì€ í• ë‹¹ëŸ‰
        # LLM ì‘ë‹µ ìºì‹± (5ë¶„ TTL)
        self.response_cache = CacheManager(default_ttl=300)
        # ëª©ì ë³„ TTL í…Œì´ë¸”
        self.purpose_ttl: Dict[str, int] = {
            "classification": 90,
            "analysis": 120,
            "news": 300,
            "knowledge": 3600,
            "response": 600,
            "general": 300,
        }
        # ì˜ë¯¸(ì„ë² ë”©) ê¸°ë°˜ ìºì‹œ
        self.semantic_cache_enabled = _np is not None and SentenceTransformer is not None
        self._semantic_model = None
        self.semantic_cache: Dict[str, List[Dict[str, Any]]] = {}  # purpose -> entries
        self.semantic_capacity_per_purpose = 500
    
    def get_llm(self, 
                model_name: Optional[str] = None, 
                temperature: float = 0.7,
                purpose: str = "general",
                **kwargs) -> ChatGoogleGenerativeAI:
        """
        Gemini LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìš©ë„ë³„ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
        """
        t0 = time.time()
        # ëª¨ë¸ëª…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        if model_name is None:
            model_name = self.default_model
        
        # ìš©ë„ë³„ ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì„¤ì •
        optimized_params = self._get_optimized_params(purpose, temperature, **kwargs)
        
        # ìºì‹œì—ì„œ í™•ì¸
        cache_key = f"{model_name}_{purpose}_{optimized_params['temperature']}_{hash(str(optimized_params))}"
        if cache_key in self.llm_cache:
            llm_cached = self.llm_cache[cache_key]
            print(f"ğŸ“¦ get_llm HIT: {model_name} ({purpose}) - {(time.time()-t0)*1000:.1f}ms")
            return llm_cached
        # API í‚¤ í™•ì¸
        google_api_key = settings.google_api_key
        if not google_api_key:
            raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # Gemini LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=google_api_key,
            **optimized_params
        )
        
        # ìºì‹œì— ì €ì¥
        self.llm_cache[cache_key] = llm
        
        print(f"ğŸ¤– Gemini LLM ë¡œë“œ: {model_name} ({purpose}, temperature: {optimized_params['temperature']}) - {(time.time()-t0)*1000:.1f}ms")
        return llm
    
    def _get_optimized_params(self, purpose: str, temperature: float, **kwargs) -> dict:
        """ìš©ë„ë³„ ìµœì í™”ëœ LLM íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        base_params = {
            "temperature": temperature,
            "top_p": 0.9,
            "top_k": 40,
            "max_output_tokens": 2048,
            **kwargs
        }
        
        # ìš©ë„ë³„ ìµœì í™”
        if purpose == "classification":
            return {
                **base_params,
                "temperature": 0.1,  # ë¶„ë¥˜ëŠ” ì¼ê´€ì„± ì¤‘ìš”
                "max_output_tokens": 512,  # êµ¬ì¡°í™”ëœ ì‘ë‹µì„ ìœ„í•´ ì¶©ë¶„í•œ í† í°
                "top_p": 0.8
            }
        elif purpose == "analysis":
            return {
                **base_params,
                "temperature": 0.3,  # ë¶„ì„ì€ ê· í˜•ì¡íŒ ì°½ì˜ì„±
                "max_output_tokens": 4096,  # ìƒì„¸í•œ ë¶„ì„ í•„ìš”
                "top_p": 0.9
            }
        elif purpose == "news":
            return {
                **base_params,
                "temperature": 0.2,  # ë‰´ìŠ¤ëŠ” ê°ê´€ì„± ì¤‘ìš”
                "max_output_tokens": 2048,
                "top_p": 0.85
            }
        elif purpose == "knowledge":
            return {
                **base_params,
                "temperature": 0.4,  # êµìœ¡ì  ì„¤ëª…ì€ ì ë‹¹í•œ ì°½ì˜ì„±
                "max_output_tokens": 3072,
                "top_p": 0.9
            }
        elif purpose == "response":
            return {
                **base_params,
                "temperature": 0.7,  # ì‘ë‹µì€ ìì—°ìŠ¤ëŸ¬ì›€ ì¤‘ìš”
                "max_output_tokens": 2048,
                "top_p": 0.9
            }
        else:  # general
            return base_params
    
    def get_default_llm(self, temperature: float = 0.7, purpose: str = "general", **kwargs) -> ChatGoogleGenerativeAI:
        """ê¸°ë³¸ Gemini LLM ë°˜í™˜"""
        return self.get_llm(model_name=None, temperature=temperature, purpose=purpose, **kwargs)
    
    def invoke_with_cache(self, llm: ChatGoogleGenerativeAI, prompt: str, purpose: str = "general") -> str:
        """LLM í˜¸ì¶œ ì‹œ ìºì‹± ì ìš© + íƒ€ì´ë° ë¡œê·¸"""
        t0 = time.time()
        cache_key = hashlib.md5(f"{prompt}_{purpose}".encode()).hexdigest()
        cached_response = self.response_cache.get(cache_key)
        if cached_response:
            print(f"ğŸ“¦ LLM ì‘ë‹µ ìºì‹œ HIT: {purpose} - {(time.time()-t0)*1000:.1f}ms")
            return cached_response
        print(f"â³ LLM í˜¸ì¶œ ì‹œì‘: {purpose}")
        response = llm.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        ttl = self.purpose_ttl.get(purpose, 300)
        self.response_cache.set(cache_key, response_text, ttl=ttl)
        print(f"ğŸ’¾ LLM ì‘ë‹µ ìºì‹œ ì €ì¥: {purpose} - {(time.time()-t0)*1000:.1f}ms (ttl={ttl}s)")
        return response_text

    # === ì˜ë¯¸(ì„ë² ë”©) ê¸°ë°˜ ìºì‹œ ===
    def _get_embedding_model(self):
        if not self.semantic_cache_enabled:
            return None
        if self._semantic_model is None:
            try:
                # ê²½ëŸ‰ ëª¨ë¸ ê¸°ë³¸ ì‚¬ìš©
                self._semantic_model = SentenceTransformer("all-MiniLM-L6-v2")
            except Exception:
                self.semantic_cache_enabled = False
                self._semantic_model = None
        return self._semantic_model

    def _semantic_lookup(self, prompt: str, purpose: str, threshold: float = 0.92) -> Optional[str]:
        if not self.semantic_cache_enabled:
            return None
        model = self._get_embedding_model()
        if model is None:
            return None
        entries = self.semantic_cache.get(purpose, [])
        if not entries:
            return None
        try:
            qv = model.encode([prompt])[0]
            best = None
            best_sim = -1.0
            for e in entries:
                sim = float(_np.dot(qv, e["vec"]) / ( _np.linalg.norm(qv) * _np.linalg.norm(e["vec"]) + 1e-9 ))
                if sim > best_sim:
                    best_sim = sim
                    best = e
            if best and best_sim >= threshold:
                print(f"ğŸ“¦ğŸ” ì˜ë¯¸ ìºì‹œ HIT: {purpose} (sim={best_sim:.3f})")
                return best["response"]
        except Exception:
            return None
        return None

    def _semantic_store(self, prompt: str, purpose: str, response_text: str):
        if not self.semantic_cache_enabled:
            return
        model = self._get_embedding_model()
        if model is None:
            return
        try:
            vec = model.encode([prompt])[0]
            entries = self.semantic_cache.setdefault(purpose, [])
            entries.append({"vec": vec, "response": response_text})
            # ìš©ëŸ‰ ì œí•œ
            if len(entries) > self.semantic_capacity_per_purpose:
                self.semantic_cache[purpose] = entries[-self.semantic_capacity_per_purpose:]
        except Exception:
            pass

    def invoke_with_semantic_cache(self, llm: ChatGoogleGenerativeAI, prompt: str, purpose: str = "general", threshold: float = 0.92) -> str:
        # 1) ì •í™• ìºì‹œ ì¡°íšŒ
        exact = self.invoke_with_cache(llm, prompt, purpose)
        if exact is not None:
            # invoke_with_cacheëŠ” miss ì‹œ LLMì„ í˜¸ì¶œí•˜ë¯€ë¡œ, ì˜ë¯¸ ìºì‹œë§Œ ë³„ë„ë¡œ ì œê³µí•˜ë ¤ë©´ ë¶„ë¦¬ í•„ìš”.
            # ì—¬ê¸°ì„œëŠ” ì˜ë¯¸ ìºì‹œ ìš°ì„  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•´ ì •í™• ìºì‹œ ì¡°íšŒë§Œ ì„ í–‰ ì²´í¬ í›„ ì§ì ‘ ì²˜ë¦¬
            cache_key = hashlib.md5(f"{prompt}_{purpose}".encode()).hexdigest()
            cached_only = self.response_cache.get(cache_key)
            if cached_only is not None:
                return cached_only
        # 2) ì˜ë¯¸ ìºì‹œ ì¡°íšŒ
        sem = self._semantic_lookup(prompt, purpose, threshold)
        if sem is not None:
            return sem
        # 3) LLM í˜¸ì¶œ í›„ ì €ì¥
        response = llm.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        ttl = self.purpose_ttl.get(purpose, 300)
        cache_key = hashlib.md5(f"{prompt}_{purpose}".encode()).hexdigest()
        self.response_cache.set(cache_key, response_text, ttl=ttl)
        self._semantic_store(prompt, purpose, response_text)
        print(f"ğŸ’¾ ì˜ë¯¸ ìºì‹œì— ì €ì¥: {purpose}")
        return response_text
    
    def clear_cache(self):
        """LLM ìºì‹œ ì´ˆê¸°í™”"""
        self.llm_cache.clear()
        self.response_cache.clear()
        print("ğŸ§¹ LLM ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì „ì—­ LLM ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
llm_manager = LLMManager()


def get_gemini_llm(model_name: Optional[str] = None, 
                   temperature: float = 0.7, 
                   **kwargs) -> ChatGoogleGenerativeAI:
    """Gemini LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (í¸ì˜ í•¨ìˆ˜)"""
    return llm_manager.get_llm(model_name, temperature, **kwargs)


def get_default_gemini_llm(temperature: float = 0.7, **kwargs) -> ChatGoogleGenerativeAI:
    """ê¸°ë³¸ Gemini LLM ë°˜í™˜ (í¸ì˜ í•¨ìˆ˜)"""
    return llm_manager.get_default_llm(temperature, **kwargs)
