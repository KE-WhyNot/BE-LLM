#!/usr/bin/env python3
"""
Gemini 2.0 Flash ì „ìš© í…ŒìŠ¤íŠ¸
ê¹”ë”í•˜ê²Œ ì •ë¦¬ëœ Geminië§Œ ì‚¬ìš©í•˜ëŠ” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
"""

import os
import time
from datetime import datetime

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Gemini ì „ìš©)
os.environ['PRIMARY_MODEL'] = 'gemini-2.0-flash-exp'
os.environ['FALLBACK_MODEL'] = 'gemini-2.0-flash-exp'
os.environ['GEMINI_MODEL'] = 'gemini-2.0-flash-exp'
os.environ['LANGSMITH_PROJECT'] = 'pr-rundown-hurry-88'
os.environ['LANGCHAIN_TRACING_V2'] = 'true'

from app.services.langgraph_enhanced.config import get_model_config, get_enhanced_settings
from app.services.langgraph_enhanced.model_selector import (
    ModelSelector, 
    TaskType, 
    select_model_for_task, 
    get_task_type_from_query
)
from app.services.langgraph_enhanced.llm_manager import get_gemini_llm, get_default_gemini_llm
from app.services.langgraph_enhanced.simplified_intelligent_workflow import simplified_intelligent_workflow


class GeminiOnlyTester:
    """Gemini ì „ìš© ì‹œìŠ¤í…œ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.settings = get_enhanced_settings()
        self.model_config = get_model_config()
        self.model_selector = ModelSelector()
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        self.test_queries = [
            "ì‚¼ì„±ì „ì ì£¼ê°€ ì•Œë ¤ì¤˜",
            "PERì´ ë­ì•¼?",
            "ì‚¼ì„±ì „ì ê¸°ìˆ ì  ë¶„ì„ê³¼ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¢…í•©í•´ì„œ íˆ¬ì ì˜ê²¬ì„ ì•Œë ¤ì¤˜",
            "Pythonìœ¼ë¡œ ì£¼ê°€ ì°¨íŠ¸ë¥¼ ê·¸ë¦¬ëŠ” ì½”ë“œë¥¼ ë§Œë“¤ì–´ì¤˜",
            "ì•ˆë…•í•˜ì„¸ìš”"
        ]
    
    def run_gemini_only_test(self):
        """Gemini ì „ìš© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ Gemini 2.0 Flash ì „ìš© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # 1. ì„¤ì • í™•ì¸
        self._check_gemini_settings()
        
        # 2. LLM ì¸ìŠ¤í„´ìŠ¤ í…ŒìŠ¤íŠ¸
        self._test_llm_instances()
        
        # 3. ëª¨ë¸ ì„ íƒ í…ŒìŠ¤íŠ¸
        self._test_model_selection()
        
        # 4. ì‹¤ì œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
        self._test_workflow()
        
        # 5. ì„±ëŠ¥ ìš”ì•½
        self._performance_summary()
    
    def _check_gemini_settings(self):
        """Gemini ì„¤ì • í™•ì¸"""
        print("\nğŸ“‹ 1. Gemini ì„¤ì • í™•ì¸")
        print("-" * 40)
        
        print(f"âœ… ê¸°ë³¸ ëª¨ë¸: {self.settings.primary_model}")
        print(f"âœ… í´ë°± ëª¨ë¸: {self.settings.fallback_model}")
        print(f"âœ… Gemini ëª¨ë¸: {self.settings.gemini_model}")
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        env_vars = ["PRIMARY_MODEL", "FALLBACK_MODEL", "GEMINI_MODEL"]
        print(f"\nğŸ”§ í™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
        for var in env_vars:
            value = os.getenv(var, "ì„¤ì • ì•ˆë¨")
            print(f"   {var}: {value}")
        
        # ëª¨ë¸ ì„¤ì • í™•ì¸
        print(f"\nâš™ï¸ ëª¨ë¸ ì„¤ì •:")
        for key, value in self.model_config.items():
            print(f"   {key}: {value}")
    
    def _test_llm_instances(self):
        """LLM ì¸ìŠ¤í„´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ¤– 2. Gemini LLM ì¸ìŠ¤í„´ìŠ¤ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        try:
            # ê¸°ë³¸ LLM í…ŒìŠ¤íŠ¸
            llm1 = get_default_gemini_llm(temperature=0.7)
            print(f"âœ… ê¸°ë³¸ LLM ë¡œë“œ ì„±ê³µ: gemini-2.0-flash-exp")
            
            # ìºì‹± í…ŒìŠ¤íŠ¸
            llm2 = get_default_gemini_llm(temperature=0.7)
            print(f"âœ… LLM ìºì‹± í™•ì¸: {llm1 is llm2}")
            
            # ë‹¤ë¥¸ temperature í…ŒìŠ¤íŠ¸
            llm3 = get_gemini_llm("gemini-2.0-flash-exp", temperature=0.1)
            print(f"âœ… ë‹¤ë¥¸ temperature LLM ë¡œë“œ: gemini-2.0-flash-exp")
            
            # ê°„ë‹¨í•œ ì‘ë‹µ í…ŒìŠ¤íŠ¸
            response = llm1.invoke("ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.")
            print(f"âœ… ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {response.content[:50]}...")
            
        except Exception as e:
            print(f"âŒ LLM ì¸ìŠ¤í„´ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _test_model_selection(self):
        """ëª¨ë¸ ì„ íƒ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ¯ 3. ëª¨ë¸ ì„ íƒ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        for i, query in enumerate(self.test_queries, 1):
            print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ {i}: {query}")
            
            # ì‘ì—… ìœ í˜• ê°ì§€
            detected_task = get_task_type_from_query(query)
            print(f"   ğŸ¯ ê°ì§€ëœ ì‘ì—…: {detected_task.value}")
            
            # ëª¨ë¸ ì„ íƒ
            selected_model = select_model_for_task(detected_task, query)
            print(f"   ğŸ¤– ì„ íƒëœ ëª¨ë¸: {selected_model}")
            
            # ì¶”ì²œ ì •ë³´
            recommendation = self.model_selector.get_model_recommendation(detected_task)
            print(f"   ğŸ’¡ ì¶”ì²œ ì´ìœ : {recommendation['reason']}")
    
    def _test_workflow(self):
        """ì‹¤ì œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ”„ 4. ì‹¤ì œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        workflow_results = []
        
        for i, query in enumerate(self.test_queries[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ“‹ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ {i}: {query}")
            
            start_time = time.time()
            
            try:
                # ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                result = simplified_intelligent_workflow.process_query(
                    query=query,
                    user_id=i,
                    session_id=f"gemini_test_{i}"
                )
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                # ê²°ê³¼ ë¶„ì„
                success = result.get("success", False)
                complexity = result.get("query_complexity", "unknown")
                confidence = result.get("confidence_score", 0.0)
                services_used = result.get("services_used", [])
                
                print(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
                print(f"   ğŸ”§ ë³µì¡ë„: {complexity}")
                print(f"   ğŸ“Š ì‹ ë¢°ë„: {confidence:.2f}")
                print(f"   ğŸ› ï¸ ì‚¬ìš©ëœ ì„œë¹„ìŠ¤: {', '.join(services_used)}")
                print(f"   âœ… ì„±ê³µ: {success}")
                
                if success and result.get("response"):
                    response_length = len(result["response"])
                    print(f"   ğŸ“ ì‘ë‹µ ê¸¸ì´: {response_length}ì")
                    print(f"   ğŸ’¬ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {result['response'][:100]}...")
                
                workflow_results.append({
                    "query": query,
                    "processing_time": processing_time,
                    "success": success,
                    "confidence": confidence,
                    "services_count": len(services_used)
                })
                
            except Exception as e:
                print(f"   âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                workflow_results.append({
                    "query": query,
                    "processing_time": time.time() - start_time,
                    "success": False,
                    "error": str(e)
                })
        
        # ê²°ê³¼ ì €ì¥
        self.workflow_results = workflow_results
    
    def _performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½"""
        print(f"\nğŸ“Š 5. ì„±ëŠ¥ ìš”ì•½")
        print("=" * 60)
        
        if hasattr(self, 'workflow_results'):
            successful_tests = sum(1 for r in self.workflow_results if r['success'])
            avg_processing_time = sum(r['processing_time'] for r in self.workflow_results) / len(self.workflow_results)
            avg_confidence = sum(r['confidence'] for r in self.workflow_results if r['success']) / max(successful_tests, 1)
            
            print(f"âœ… ì„±ê³µë¥ : {successful_tests}/{len(self.workflow_results)} ({successful_tests/len(self.workflow_results)*100:.1f}%)")
            print(f"â±ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
            print(f"ğŸ“Š í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}")
        
        print(f"\nğŸ¯ Gemini 2.0 Flash ì „ìš© ì‹œìŠ¤í…œ íŠ¹ì§•:")
        print("   ğŸš€ ë¹ ë¥¸ ì‘ë‹µ ì†ë„ (1ì´ˆ ì´ë‚´)")
        print("   ğŸ’° ì €ë ´í•œ ë¹„ìš© (ë¬´ë£Œ í• ë‹¹ëŸ‰ ë§ìŒ)")
        print("   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ê¸ˆìœµ ìš©ì–´ íŠ¹í™”")
        print("   ğŸ“ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (1M í† í°)")
        print("   ğŸ¯ ë‹¨ìˆœí•˜ê³  ê¹”ë”í•œ êµ¬ì¡°")
        
        print(f"\nâš™ï¸ ìµœì í™”ëœ ì„¤ì •:")
        print("   PRIMARY_MODEL=gemini-2.0-flash-exp")
        print("   FALLBACK_MODEL=gemini-2.0-flash-exp")
        print("   GEMINI_MODEL=gemini-2.0-flash-exp")
        print("   (GPT ê´€ë ¨ ì„¤ì • ì œê±°ë¨)")


if __name__ == "__main__":
    tester = GeminiOnlyTester()
    tester.run_gemini_only_test()
