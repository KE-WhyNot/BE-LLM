"""
ì‹¤ì œ ì„±ëŠ¥ ì¸¡ì • ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

ê¸°ì¡´ ì‹œìŠ¤í…œ vs LangGraph Enhanced ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ
"""

import time
import asyncio
import statistics
from typing import Dict, List, Any
from datetime import datetime
import json

# ê¸°ì¡´ ì‹œìŠ¤í…œ import
from app.services.chatbot.chatbot_service import FinancialChatbotService
from app.services.workflow_components.query_classifier_service import QueryClassifierService

# LangGraph Enhanced ì‹œìŠ¤í…œ import
from app.services.langgraph_enhanced.enhanced_financial_workflow import EnhancedFinancialWorkflow
from app.services.langgraph_enhanced.performance_monitor import PerformanceMonitor

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.existing_chatbot = FinancialChatbotService()
        self.enhanced_workflow = EnhancedFinancialWorkflow()
        self.performance_monitor = PerformanceMonitor()
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        self.test_queries = [
            "ì‚¼ì„±ì „ì ì£¼ê°€ ì•Œë ¤ì¤˜",
            "ì• í”Œ ì£¼ì‹ ë¶„ì„í•´ì¤˜", 
            "íˆ¬ì ì „ëµ ì¶”ì²œí•´ì¤˜",
            "PERì´ ë­ì•¼?",
            "ìµœì‹  ê¸ˆìœµ ë‰´ìŠ¤ ë³´ì—¬ì¤˜",
            "í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?",
            "ì‹œì¥ ì „ë§ì€ ì–´ë•Œ?",
            "ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ë²• ì•Œë ¤ì¤˜"
        ]
    
    async def test_existing_system(self) -> Dict[str, Any]:
        """ê¸°ì¡´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì •"""
        print("ğŸ” ê¸°ì¡´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        results = {
            "system": "ê¸°ì¡´ ì‹œìŠ¤í…œ",
            "total_queries": len(self.test_queries),
            "response_times": [],
            "success_count": 0,
            "error_count": 0,
            "errors": [],
            "start_time": datetime.now().isoformat()
        }
        
        for i, query in enumerate(self.test_queries, 1):
            print(f"  ğŸ“ í…ŒìŠ¤íŠ¸ {i}/{len(self.test_queries)}: {query}")
            
            start_time = time.time()
            try:
                # ê¸°ì¡´ ì‹œìŠ¤í…œ í˜¸ì¶œ
                response = await self.existing_chatbot.process_query({
                    "query": query,
                    "user_id": "benchmark_test"
                })
                
                end_time = time.time()
                response_time = end_time - start_time
                
                results["response_times"].append(response_time)
                results["success_count"] += 1
                
                print(f"    âœ… ì„±ê³µ ({response_time:.2f}ì´ˆ)")
                
            except Exception as e:
                end_time = time.time()
                response_time = end_time - start_time
                
                results["response_times"].append(response_time)
                results["error_count"] += 1
                results["errors"].append({
                    "query": query,
                    "error": str(e),
                    "response_time": response_time
                })
                
                print(f"    âŒ ì‹¤íŒ¨ ({response_time:.2f}ì´ˆ): {str(e)}")
        
        # í†µê³„ ê³„ì‚°
        if results["response_times"]:
            results["avg_response_time"] = statistics.mean(results["response_times"])
            results["min_response_time"] = min(results["response_times"])
            results["max_response_time"] = max(results["response_times"])
            results["success_rate"] = results["success_count"] / results["total_queries"] * 100
        
        results["end_time"] = datetime.now().isoformat()
        return results
    
    async def test_enhanced_system(self) -> Dict[str, Any]:
        """LangGraph Enhanced ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì •"""
        print("ğŸš€ LangGraph Enhanced ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        results = {
            "system": "LangGraph Enhanced",
            "total_queries": len(self.test_queries),
            "response_times": [],
            "success_count": 0,
            "error_count": 0,
            "errors": [],
            "start_time": datetime.now().isoformat()
        }
        
        for i, query in enumerate(self.test_queries, 1):
            print(f"  ğŸ“ í…ŒìŠ¤íŠ¸ {i}/{len(self.test_queries)}: {query}")
            
            start_time = time.time()
            try:
                # Enhanced ì‹œìŠ¤í…œ í˜¸ì¶œ
                response = await self.enhanced_workflow.process_query({
                    "query": query,
                    "user_id": "benchmark_test"
                })
                
                end_time = time.time()
                response_time = end_time - start_time
                
                results["response_times"].append(response_time)
                results["success_count"] += 1
                
                print(f"    âœ… ì„±ê³µ ({response_time:.2f}ì´ˆ)")
                
            except Exception as e:
                end_time = time.time()
                response_time = end_time - start_time
                
                results["response_times"].append(response_time)
                results["error_count"] += 1
                results["errors"].append({
                    "query": query,
                    "error": str(e),
                    "response_time": response_time
                })
                
                print(f"    âŒ ì‹¤íŒ¨ ({response_time:.2f}ì´ˆ): {str(e)}")
        
        # í†µê³„ ê³„ì‚°
        if results["response_times"]:
            results["avg_response_time"] = statistics.mean(results["response_times"])
            results["min_response_time"] = min(results["response_times"])
            results["max_response_time"] = max(results["response_times"])
            results["success_rate"] = results["success_count"] / results["total_queries"] * 100
        
        results["end_time"] = datetime.now().isoformat()
        return results
    
    def compare_results(self, existing_results: Dict[str, Any], enhanced_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë‘ ì‹œìŠ¤í…œ ê²°ê³¼ ë¹„êµ"""
        comparison = {
            "test_date": datetime.now().isoformat(),
            "total_queries": len(self.test_queries),
            "comparison": {}
        }
        
        # ì‘ë‹µ ì‹œê°„ ë¹„êµ
        if "avg_response_time" in existing_results and "avg_response_time" in enhanced_results:
            time_improvement = ((existing_results["avg_response_time"] - enhanced_results["avg_response_time"]) / existing_results["avg_response_time"]) * 100
            comparison["comparison"]["response_time"] = {
                "existing": existing_results["avg_response_time"],
                "enhanced": enhanced_results["avg_response_time"],
                "improvement_percent": time_improvement
            }
        
        # ì„±ê³µë¥  ë¹„êµ
        if "success_rate" in existing_results and "success_rate" in enhanced_results:
            success_improvement = enhanced_results["success_rate"] - existing_results["success_rate"]
            comparison["comparison"]["success_rate"] = {
                "existing": existing_results["success_rate"],
                "enhanced": enhanced_results["success_rate"],
                "improvement_percent": success_improvement
            }
        
        # ì—ëŸ¬ìœ¨ ë¹„êµ
        existing_error_rate = existing_results["error_count"] / existing_results["total_queries"] * 100
        enhanced_error_rate = enhanced_results["error_count"] / enhanced_results["total_queries"] * 100
        error_improvement = existing_error_rate - enhanced_error_rate
        
        comparison["comparison"]["error_rate"] = {
            "existing": existing_error_rate,
            "enhanced": enhanced_error_rate,
            "improvement_percent": error_improvement
        }
        
        return comparison
    
    def print_results(self, existing_results: Dict[str, Any], enhanced_results: Dict[str, Any], comparison: Dict[str, Any]):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*60)
        
        print(f"\nğŸ” ê¸°ì¡´ ì‹œìŠ¤í…œ:")
        print(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {existing_results.get('avg_response_time', 0):.2f}ì´ˆ")
        print(f"  - ì„±ê³µë¥ : {existing_results.get('success_rate', 0):.1f}%")
        print(f"  - ì—ëŸ¬ìœ¨: {existing_results['error_count']}/{existing_results['total_queries']} ({existing_results['error_count']/existing_results['total_queries']*100:.1f}%)")
        
        print(f"\nğŸš€ LangGraph Enhanced:")
        print(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {enhanced_results.get('avg_response_time', 0):.2f}ì´ˆ")
        print(f"  - ì„±ê³µë¥ : {enhanced_results.get('success_rate', 0):.1f}%")
        print(f"  - ì—ëŸ¬ìœ¨: {enhanced_results['error_count']}/{enhanced_results['total_queries']} ({enhanced_results['error_count']/enhanced_results['total_queries']*100:.1f}%)")
        
        print(f"\nğŸ“ˆ ê°œì„  íš¨ê³¼:")
        if "response_time" in comparison["comparison"]:
            time_improvement = comparison["comparison"]["response_time"]["improvement_percent"]
            print(f"  - ì‘ë‹µ ì‹œê°„: {time_improvement:+.1f}%")
        
        if "success_rate" in comparison["comparison"]:
            success_improvement = comparison["comparison"]["success_rate"]["improvement_percent"]
            print(f"  - ì„±ê³µë¥ : {success_improvement:+.1f}%")
        
        if "error_rate" in comparison["comparison"]:
            error_improvement = comparison["comparison"]["error_rate"]["improvement_percent"]
            print(f"  - ì—ëŸ¬ìœ¨: {error_improvement:+.1f}%")
        
        print("="*60)
    
    async def run_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜: {len(self.test_queries)}ê°œ")
        print(f"ğŸ“… í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        existing_results = await self.test_existing_system()
        
        print("\n" + "-"*40)
        
        # Enhanced ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        enhanced_results = await self.test_enhanced_system()
        
        # ê²°ê³¼ ë¹„êµ
        comparison = self.compare_results(existing_results, enhanced_results)
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_results(existing_results, enhanced_results, comparison)
        
        # ê²°ê³¼ ì €ì¥
        results_data = {
            "existing_system": existing_results,
            "enhanced_system": enhanced_results,
            "comparison": comparison
        }
        
        with open("benchmark_results.json", "w", encoding="utf-8") as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ benchmark_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return results_data

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    benchmark = PerformanceBenchmark()
    results = await benchmark.run_benchmark()
    return results

if __name__ == "__main__":
    asyncio.run(main())
