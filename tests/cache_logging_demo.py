import time


from app.services.langgraph_enhanced.agents.base_agent import BaseAgent


class FakeLLMManager:
    """LLM í˜¸ì¶œ ìºì‹œ/ì§€ì—°ì„ ëª¨ì‚¬í•˜ëŠ” í˜ì´í¬ ë§¤ë‹ˆì €"""
    def __init__(self):
        self._cache = {}

    def get_llm(self, *args, **kwargs):
        return object()  # ë”ë¯¸ ê°ì²´

    def invoke_with_cache(self, llm, prompt: str, purpose: str = "general") -> str:
        key = (prompt, purpose)
        if key in self._cache:
            print("ğŸ“¦ [FAKE] cache hit")
            return self._cache[key]
        # ì²« í˜¸ì¶œì—ëŠ” ì¸ìœ„ì  ì§€ì—°
        time.sleep(0.2)
        resp = f"FAKE_RESPONSE[{purpose}] for {prompt[:30]}"
        self._cache[key] = resp
        print("ğŸ†• [FAKE] cache set")
        return resp


class MinimalAgent(BaseAgent):
    def __init__(self):
        # super().__init__ í˜¸ì¶œì„ í”¼í•˜ê³  ì§ì ‘ ì£¼ì…(ì‹¤ì œ LLM ì‚¬ìš© ë°©ì§€)
        self.llm_manager = FakeLLMManager()
        self.llm = self.llm_manager.get_llm(purpose="analysis")
        self.purpose = "analysis"
        self.agent_name = "minimal_agent"

    def get_prompt_template(self) -> str:
        return "í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {msg}"

    def process(self, msg: str):
        prompt = self.get_prompt_template().format(msg=msg)
        # 1ì°¨ í˜¸ì¶œ(ìºì‹œ ë¯¸ìŠ¤ â†’ ì§€ì—°)
        r1 = self.invoke_llm_with_cache(prompt, purpose=self.purpose, log_label="demo_call")
        # 2ì°¨ í˜¸ì¶œ(ìºì‹œ íˆíŠ¸ â†’ ì¦‰ì‹œ)
        r2 = self.invoke_llm_with_cache(prompt, purpose=self.purpose, log_label="demo_call")
        return r1, r2


def main():
    agent = MinimalAgent()
    t0 = time.time()
    r1, r2 = agent.process("ì•ˆë…•í•˜ì„¸ìš”")
    dt = (time.time() - t0) * 1000
    print(f"\nâ± ì „ì²´ ì‹¤í–‰ ì‹œê°„: {dt:.1f}ms")
    print(f"ì‘ë‹µ ë™ì¼ì„± í™•ì¸: {r1 == r2}")


if __name__ == "__main__":
    main()


