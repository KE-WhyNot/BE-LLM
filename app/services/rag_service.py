import os
import chromadb
from chromadb.config import Settings
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.schema import Document
from typing import List, Dict, Any
import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import json

class FinancialRAGService:
    def __init__(self, persist_directory: str = "./chroma_db"):
        """
        ê¸ˆìœµ ì „ë¬¸ê°€ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        """
        self.persist_directory = persist_directory
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        self.vectorstore = None
        self._initialize_vectorstore()
    
    def _initialize_vectorstore(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            print("ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    def add_financial_documents(self, documents: List[Document]):
        """ê¸ˆìœµ ë¬¸ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€"""
        if not documents:
            return
        
        # ë¬¸ì„œ ë¶„í• 
        split_docs = self.text_splitter.split_documents(documents)
        
        # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        self.vectorstore.add_documents(split_docs)
        # persist ë©”ì„œë“œëŠ” ìë™ìœ¼ë¡œ í˜¸ì¶œë¨
        print(f"{len(split_docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    
    def get_financial_data(self, symbol: str, period: str = "1mo") -> Dict[str, Any]:
        """Yahoo Financeì—ì„œ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            ticker = yf.Ticker(symbol)
            hist = ticker.history(period=period)
            info = ticker.info
            
            if hist.empty:
                return {"error": f"{symbol}ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            # ìµœì‹  ë°ì´í„°
            latest = hist.iloc[-1]
            previous = hist.iloc[-2] if len(hist) > 1 else latest
            
            # ê°€ê²© ë³€í™” ê³„ì‚°
            price_change = latest['Close'] - previous['Close']
            price_change_percent = (price_change / previous['Close']) * 100
            
            return {
                "symbol": symbol,
                "current_price": round(latest['Close'], 2),
                "price_change": round(price_change, 2),
                "price_change_percent": round(price_change_percent, 2),
                "volume": int(latest['Volume']),
                "high": round(latest['High'], 2),
                "low": round(latest['Low'], 2),
                "open": round(latest['Open'], 2),
                "company_name": info.get('longName', symbol),
                "sector": info.get('sector', 'Unknown'),
                "market_cap": info.get('marketCap', 'Unknown'),
                "pe_ratio": info.get('trailingPE', 'Unknown'),
                "dividend_yield": info.get('dividendYield', 'Unknown'),
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {"error": f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"}
    
    def _extract_stock_symbols(self, query: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ ì£¼ì‹ëª…ì„ ì¶”ì¶œí•˜ì—¬ ì‹¬ë³¼ë¡œ ë³€í™˜"""
        # ì£¼ìš” í•œêµ­ ì£¼ì‹ ë§¤í•‘
        stock_mapping = {
            # ì‚¼ì„± ê´€ë ¨
            "ì‚¼ì„±": ["005930.KS", "SSNLF"],
            "ì‚¼ì„±ì „ì": ["005930.KS", "SSNLF"],
            "samsung": ["005930.KS", "SSNLF"],
            
            # í•˜ì´ë‹‰ìŠ¤ ê´€ë ¨
            "í•˜ì´ë‹‰ìŠ¤": ["000660.KS", "HXSCL"],
            "skí•˜ì´ë‹‰ìŠ¤": ["000660.KS", "HXSCL"],
            "hynix": ["000660.KS", "HXSCL"],
            
            # LG ê´€ë ¨
            "lg": ["003550.KS", "LPLIY"],
            "lgì „ì": ["003550.KS", "LPLIY"],
            "lgí™”í•™": ["051910.KS", "LGCHEM"],
            
            # ë„¤ì´ë²„ ê´€ë ¨
            "ë„¤ì´ë²„": ["035420.KS", "NHNCF"],
            "naver": ["035420.KS", "NHNCF"],
            
            # ì¹´ì¹´ì˜¤ ê´€ë ¨
            "ì¹´ì¹´ì˜¤": ["035720.KS", "KAKAO"],
            "kakao": ["035720.KS", "KAKAO"],
            
            # í˜„ëŒ€ì°¨ ê´€ë ¨
            "í˜„ëŒ€ì°¨": ["005380.KS", "HYMTF"],
            "í˜„ëŒ€ìë™ì°¨": ["005380.KS", "HYMTF"],
            "hyundai": ["005380.KS", "HYMTF"],
            
            # ê¸°ì•„ ê´€ë ¨
            "ê¸°ì•„": ["000270.KS", "KIMTF"],
            "kia": ["000270.KS", "KIMTF"],
            
            # SKí…”ë ˆì½¤ ê´€ë ¨
            "skí…”ë ˆì½¤": ["017670.KS", "SKM"],
            "sktelecom": ["017670.KS", "SKM"],
            
            # POSCO ê´€ë ¨
            "í¬ìŠ¤ì½”": ["005490.KS", "PKX"],
            "posco": ["005490.KS", "PKX"],
            
            # ì‹œì¥ ì§€ìˆ˜
            "ì½”ìŠ¤í”¼": ["^KS11"],
            "kospi": ["^KS11"],
            "ì½”ìŠ¤ë‹¥": ["^KQ11"],
            "kosdaq": ["^KQ11"],
            "ë‚˜ìŠ¤ë‹¥": ["^IXIC"],
            "nasdaq": ["^IXIC"],
            "ë‹¤ìš°": ["^DJI"],
            "dow": ["^DJI"],
            "s&p500": ["^GSPC"],
            "sp500": ["^GSPC"]
        }
        
        found_symbols = []
        query_lower = query.lower()
        
        for keyword, symbols in stock_mapping.items():
            if keyword in query_lower:
                found_symbols.extend(symbols)
        
        return found_symbols

    def get_financial_news(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Yahoo Finance RSSì—ì„œ ì‹¤ì œ ê¸ˆìœµ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        news_list = []
        
        try:
            import feedparser
            from datetime import datetime
            
            # ì§ˆë¬¸ì—ì„œ ì£¼ì‹ ì‹¬ë³¼ ìë™ ì¶”ì¶œ
            stock_symbols = self._extract_stock_symbols(query)
            
            # RSS URL ìƒì„±
            rss_urls = []
            if stock_symbols:
                print(f"ğŸ” ê°ì§€ëœ ì£¼ì‹ ì‹¬ë³¼: {stock_symbols}")
                for symbol in stock_symbols:
                    rss_urls.append(f"https://feeds.finance.yahoo.com/rss/2.0/headline?s={symbol}&region=US&lang=en-US")
            else:
                # ì¼ë°˜ì ì¸ ê¸ˆìœµ ë‰´ìŠ¤
                print("ğŸ“° ì¼ë°˜ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰")
                rss_urls = [
                    "https://feeds.finance.yahoo.com/rss/2.0/headline",
                    "https://feeds.finance.yahoo.com/rss/2.0/headline?s=^GSPC&region=US&lang=en-US"
                ]
            
            for rss_url in rss_urls:
                try:
                    print(f"RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {rss_url}")
                    feed = feedparser.parse(rss_url)
                    
                    if feed.bozo:
                        print(f"RSS í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜: {feed.bozo_exception}")
                        continue
                    
                    for entry in feed.entries[:max_results//len(rss_urls)]:
                        title = entry.get('title', '').strip()
                        summary = entry.get('summary', '').strip()
                        link = entry.get('link', '').strip()
                        published_raw = entry.get('published', datetime.now().isoformat())
                        
                        # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (ë‚ ì§œë§Œ í‘œì‹œ)
                        try:
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                published_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
                            else:
                                published_date = datetime.now().strftime('%Y-%m-%d')
                        except:
                            published_date = datetime.now().strftime('%Y-%m-%d')
                        
                        # ìœ íš¨í•œ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
                        if title and len(title) > 5 and link:
                            # ë‰´ìŠ¤ ë¶„ì„ ë° ì˜í–¥ë„ ì˜ˆì¸¡
                            impact_analysis = self._analyze_news_impact(title, summary)
                            
                            news_list.append({
                                "title": title,
                                "summary": summary if summary else f"Yahoo Finance: {title}",
                                "url": link,
                                "published": published_date,
                                "impact_analysis": impact_analysis
                            })
                    
                except Exception as e:
                    print(f"RSS URL {rss_url} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"Yahoo Finance RSSì—ì„œ {len(news_list)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return news_list[:max_results]
            
        except Exception as e:
            print(f"ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            return [
                {
                    "title": f"{query} ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ 1",
                    "summary": f"{query}ì— ëŒ€í•œ ë¶„ì„ê°€ ì˜ê²¬ê³¼ ì‹œì¥ ì „ë§",
                    "url": "https://example.com/news1",
                    "published": datetime.now().isoformat()
                },
                {
                    "title": f"{query} ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ 2", 
                    "summary": f"{query}ì˜ ì‹¤ì  ë°œí‘œì™€ íˆ¬ìì ë°˜ì‘",
                    "url": "https://example.com/news2",
                    "published": datetime.now().isoformat()
                }
            ][:max_results]
    
    def _analyze_news_impact(self, title: str, summary: str) -> Dict[str, Any]:
        """ë‰´ìŠ¤ì˜ ì‹œì¥ ì˜í–¥ë„ ë¶„ì„"""
        try:
            # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ë¶„ì„
            positive_keywords = [
                'ìƒìŠ¹', 'ì¦ê°€', 'ì„±ì¥', 'í˜¸ì¬', 'ê¸ì •', 'ê°œì„ ', 'í™•ëŒ€', 'íˆ¬ì', 'ë§¤ìˆ˜',
                'rise', 'increase', 'growth', 'positive', 'improve', 'expand', 'buy'
            ]
            negative_keywords = [
                'í•˜ë½', 'ê°ì†Œ', 'ìœ„í—˜', 'ì•…ì¬', 'ë¶€ì •', 'ì•…í™”', 'ì¶•ì†Œ', 'ë§¤ë„', 'ì†ì‹¤',
                'fall', 'decrease', 'risk', 'negative', 'worse', 'reduce', 'sell', 'loss'
            ]
            
            text = (title + " " + summary).lower()
            
            positive_count = sum(1 for keyword in positive_keywords if keyword in text)
            negative_count = sum(1 for keyword in negative_keywords if keyword in text)
            
            # ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°
            if positive_count > negative_count:
                impact_score = min(positive_count * 20, 100)
                impact_direction = "ê¸ì •ì "
            elif negative_count > positive_count:
                impact_score = min(negative_count * 20, 100)
                impact_direction = "ë¶€ì •ì "
            else:
                impact_score = 50
                impact_direction = "ì¤‘ë¦½ì "
            
            # ì‹œì¥ ì˜í–¥ë„ ì˜ˆì¸¡
            if impact_score >= 80:
                market_impact = "ë†’ìŒ - ì£¼ê°€ì— í° ì˜í–¥ ì˜ˆìƒ"
            elif impact_score >= 60:
                market_impact = "ì¤‘ê°„ - ì£¼ê°€ì— ì ë‹¹í•œ ì˜í–¥ ì˜ˆìƒ"
            else:
                market_impact = "ë‚®ìŒ - ì£¼ê°€ì— ë¯¸ë¯¸í•œ ì˜í–¥ ì˜ˆìƒ"
            
            return {
                "impact_score": impact_score,
                "impact_direction": impact_direction,
                "market_impact": market_impact,
                "positive_keywords": positive_count,
                "negative_keywords": negative_count
            }
            
        except Exception as e:
            return {
                "impact_score": 50,
                "impact_direction": "ì¤‘ë¦½ì ",
                "market_impact": "ë¶„ì„ ë¶ˆê°€",
                "positive_keywords": 0,
                "negative_keywords": 0
            }
    
    def get_news_content_from_url(self, url: str) -> Dict[str, Any]:
        """ë‰´ìŠ¤ URLì—ì„œ ì‹¤ì œ ë‚´ìš©ì„ ìŠ¤í¬ë˜í•‘"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ (ë” ë§ì€ ì„ íƒì ì¶”ê°€)
            title = ""
            title_selectors = [
                'h1', '.caas-title-wrapper h1', '.headline', 'title',
                '[data-module="ArticleHeader"] h1', '.caas-title',
                'h1[data-testid="headline"]', '.caas-title-wrapper'
            ]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    break
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ë” ë§ì€ ì„ íƒìì™€ í´ë°± ë¡œì§)
            content = ""
            content_selectors = [
                '.caas-body', '.article-body', '.story-body', 
                '.content', '.article-content', 'article p',
                '[data-module="ArticleBody"]', '.caas-body p',
                '.caas-body div', 'article div', '.story-content'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ëª¨ë“  p íƒœê·¸ì™€ div íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹¨
                    paragraphs = content_elem.find_all(['p', 'div'])
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                    if len(content) > 100:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¤‘ë‹¨
                        break
            
            # í´ë°±: ì „ì²´ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if not content or len(content) < 100:
                # ëª¨ë“  p íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                all_paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text(strip=True) for p in all_paragraphs if p.get_text(strip=True)])
                
                # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ div íƒœê·¸ë„ í¬í•¨
                if len(content) < 100:
                    all_divs = soup.find_all('div')
                    div_text = ' '.join([d.get_text(strip=True) for d in all_divs if d.get_text(strip=True)])
                    content = content + ' ' + div_text
            
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
            author = ""
            author_elem = soup.select_one('[data-module="ArticleHeader"] .caas-author-byline-collapse')
            if author_elem:
                author = author_elem.get_text(strip=True)
            
            published_date = ""
            date_elem = soup.select_one('time, .caas-attr-time-style')
            if date_elem:
                published_date = date_elem.get_text(strip=True)
            
            return {
                "title": title,
                "content": content,
                "author": author,
                "published_date": published_date,
                "url": url,
                "success": True
            }
            
        except Exception as e:
            return {
                "error": f"ë‰´ìŠ¤ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}",
                "url": url,
                "success": False
            }
    
    def analyze_news_impact(self, news_content: str, title: str = "") -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ì‹ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ í‰ê°€"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
            positive_keywords = [
                'ìƒìŠ¹', 'ì¦ê°€', 'ì„±ì¥', 'ê°œì„ ', 'í˜¸ì¬', 'ê¸ì •ì ', 'ê°•ì„¸', 'ëŒíŒŒ',
                'rise', 'increase', 'growth', 'improve', 'positive', 'bullish', 'breakthrough'
            ]
            
            negative_keywords = [
                'í•˜ë½', 'ê°ì†Œ', 'ìœ„ê¸°', 'ì•…í™”', 'ë¶€ì¬', 'ë¶€ì •ì ', 'ì•½ì„¸', 'í­ë½',
                'fall', 'decrease', 'crisis', 'worse', 'negative', 'bearish', 'crash'
            ]
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            content_lower = news_content.lower()
            title_lower = title.lower()
            combined_text = f"{title_lower} {content_lower}"
            
            positive_count = sum(1 for keyword in positive_keywords if keyword in combined_text)
            negative_count = sum(1 for keyword in negative_keywords if keyword in combined_text)
            
            # ì˜í–¥ë„ ê³„ì‚°
            total_keywords = positive_count + negative_count
            if total_keywords == 0:
                impact_score = 0
                sentiment = "ì¤‘ë¦½"
            else:
                impact_score = (positive_count - negative_count) / total_keywords
                if impact_score > 0.3:
                    sentiment = "ê¸ì •ì "
                elif impact_score < -0.3:
                    sentiment = "ë¶€ì •ì "
                else:
                    sentiment = "ì¤‘ë¦½"
            
            # ì˜í–¥ë„ ë ˆë²¨ ê²°ì •
            if abs(impact_score) > 0.7:
                impact_level = "ë†’ìŒ"
            elif abs(impact_score) > 0.4:
                impact_level = "ì¤‘ê°„"
            else:
                impact_level = "ë‚®ìŒ"
            
            return {
                "sentiment": sentiment,
                "impact_score": impact_score,
                "impact_level": impact_level,
                "positive_keywords_found": positive_count,
                "negative_keywords_found": negative_count,
                "analysis": f"ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼: {sentiment}ì  ì˜í–¥ ({impact_level} ìˆ˜ì¤€)"
            }
            
        except Exception as e:
            return {
                "error": f"ë‰´ìŠ¤ ì˜í–¥ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                "sentiment": "ë¶„ì„ ë¶ˆê°€",
                "impact_score": 0,
                "impact_level": "ì•Œ ìˆ˜ ì—†ìŒ"
            }
    
    def summarize_news_content(self, news_content: str, title: str = "", max_length: int = 300) -> str:
        """ë‰´ìŠ¤ ë‚´ìš©ì„ ìš”ì•½"""
        try:
            if not news_content or len(news_content.strip()) < 50:
                return "ë‰´ìŠ¤ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê°„ë‹¨í•œ ìš”ì•½ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
            sentences = news_content.split('.')
            
            # ì¤‘ìš”í•œ ë¬¸ì¥ë“¤ ì„ íƒ (ê¸¸ì´ì™€ í‚¤ì›Œë“œ ê¸°ë°˜)
            important_keywords = [
                'ì£¼ê°€', 'ì£¼ì‹', 'íˆ¬ì', 'ìˆ˜ìµ', 'ì‹¤ì ', 'ë§¤ì¶œ', 'ì´ìµ', 'ì†ì‹¤',
                'stock', 'price', 'investment', 'revenue', 'profit', 'loss',
                'ì¦ê°€', 'ê°ì†Œ', 'ìƒìŠ¹', 'í•˜ë½', 'ì„±ì¥', 'ìœ„ê¸°'
            ]
            
            scored_sentences = []
            for sentence in sentences:
                if len(sentence.strip()) > 20:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                    score = 0
                    sentence_lower = sentence.lower()
                    
                    # í‚¤ì›Œë“œ ì ìˆ˜
                    for keyword in important_keywords:
                        if keyword in sentence_lower:
                            score += 1
                    
                    # ê¸¸ì´ ì ìˆ˜ (ì ë‹¹í•œ ê¸¸ì´ì˜ ë¬¸ì¥ ì„ í˜¸)
                    if 50 <= len(sentence) <= 200:
                        score += 1
                    
                    scored_sentences.append((score, sentence.strip()))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ë¬¸ì¥ë“¤ ì„ íƒ
            scored_sentences.sort(reverse=True)
            summary_sentences = [sent[1] for sent in scored_sentences[:3]]
            
            summary = '. '.join(summary_sentences)
            if summary and not summary.endswith('.'):
                summary += '.'
            
            # ê¸¸ì´ ì œí•œ
            if len(summary) > max_length:
                summary = summary[:max_length] + "..."
            
            return summary if summary else "ë‰´ìŠ¤ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def create_financial_knowledge_base(self):
        """ê¸ˆìœµ ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±"""
        financial_documents = []
        
        # ê¸°ë³¸ ê¸ˆìœµ ì§€ì‹ ë¬¸ì„œë“¤
        basic_knowledge = [
            Document(
                page_content="""
                ì£¼ì‹ íˆ¬ìì˜ ê¸°ë³¸ ì›ì¹™:
                1. ë¶„ì‚°íˆ¬ì: ì—¬ëŸ¬ ì¢…ëª©ì— íˆ¬ìí•˜ì—¬ ë¦¬ìŠ¤í¬ë¥¼ ë¶„ì‚°ì‹œí‚µë‹ˆë‹¤.
                2. ì¥ê¸°íˆ¬ì: ë‹¨ê¸° ë³€ë™ì„±ë³´ë‹¤ ì¥ê¸° ì„±ì¥ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.
                3. ê¸°ë³¸ì  ë¶„ì„: ê¸°ì—…ì˜ ì¬ë¬´ìƒíƒœì™€ ì‚¬ì—…ëª¨ë¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.
                4. ê¸°ìˆ ì  ë¶„ì„: ì°¨íŠ¸ì™€ ê±°ë˜ëŸ‰ì„ í†µí•´ ë§¤ë§¤ íƒ€ì´ë°ì„ ê²°ì •í•©ë‹ˆë‹¤.
                5. ë¦¬ìŠ¤í¬ ê´€ë¦¬: ì†ì‹¤ì„ ì œí•œí•˜ê³  ìˆ˜ìµì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
                """,
                metadata={"source": "investment_basics", "type": "basic_knowledge"}
            ),
            Document(
                page_content="""
                ì¬ë¬´ì œí‘œ ë¶„ì„ ë°©ë²•:
                1. ì†ìµê³„ì‚°ì„œ: ë§¤ì¶œ, ë¹„ìš©, ìˆœì´ìµì„ í™•ì¸í•˜ì—¬ ìˆ˜ìµì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
                2. ëŒ€ì°¨ëŒ€ì¡°í‘œ: ìì‚°, ë¶€ì±„, ìë³¸ì„ í™•ì¸í•˜ì—¬ ì¬ë¬´ì•ˆì •ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
                3. í˜„ê¸ˆíë¦„í‘œ: í˜„ê¸ˆì˜ ìœ ì…ê³¼ ìœ ì¶œì„ í™•ì¸í•˜ì—¬ í˜„ê¸ˆì°½ì¶œëŠ¥ë ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
                4. ì£¼ìš” ì¬ë¬´ë¹„ìœ¨: ROE, ROA, PER, PBR ë“±ì„ í†µí•´ ê¸°ì—…ê°€ì¹˜ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
                """,
                metadata={"source": "financial_analysis", "type": "analysis_method"}
            ),
            Document(
                page_content="""
                ì‹œì¥ ì§€ìˆ˜ì™€ ì„¹í„°:
                1. KOSPI: í•œêµ­ ì¢…í•©ì£¼ê°€ì§€ìˆ˜, ëŒ€í˜•ì£¼ ì¤‘ì‹¬ì˜ ì‹œì¥ ì „ì²´ ì§€í‘œ
                2. KOSDAQ: í•œêµ­ ê±°ë˜ì†Œ ì½”ìŠ¤ë‹¥ì‹œì¥ ì§€ìˆ˜, ì¤‘ì†Œí˜•ì£¼ ì¤‘ì‹¬
                3. ì£¼ìš” ì„¹í„°: ê¸°ìˆ , ê¸ˆìœµ, í—¬ìŠ¤ì¼€ì–´, ì—ë„ˆì§€, ì†Œë¹„ì¬, ì‚°ì—…ì¬ ë“±
                4. ì„¹í„° ë¡œí…Œì´ì…˜: ê²½ì œ ì‚¬ì´í´ì— ë”°ë¥¸ ì„¹í„°ë³„ íˆ¬ì ì „ëµ
                """,
                metadata={"source": "market_indices", "type": "market_knowledge"}
            )
        ]
        
        financial_documents.extend(basic_knowledge)
        
        # ì£¼ìš” ì¢…ëª© ì •ë³´ ì¶”ê°€
        major_stocks = ["005930.KS", "000660.KS", "035420.KS", "207940.KS", "006400.KS"]
        for symbol in major_stocks:
            stock_data = self.get_financial_data(symbol)
            if "error" not in stock_data:
                stock_doc = Document(
                    page_content=f"""
                    {stock_data['company_name']} ({symbol}) ì •ë³´:
                    í˜„ì¬ê°€: {stock_data['current_price']}ì›
                    ì „ì¼ëŒ€ë¹„: {stock_data['price_change']}ì› ({stock_data['price_change_percent']}%)
                    ê±°ë˜ëŸ‰: {stock_data['volume']:,}ì£¼
                    ì‹œê°€ì´ì•¡: {stock_data['market_cap']}
                    PER: {stock_data['pe_ratio']}
                    ë°°ë‹¹ìˆ˜ìµë¥ : {stock_data['dividend_yield']}
                    ì„¹í„°: {stock_data['sector']}
                    """,
                    metadata={"source": f"stock_{symbol}", "type": "stock_info", "symbol": symbol}
                )
                financial_documents.append(stock_doc)
        
        # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        self.add_financial_documents(financial_documents)
        print(f"ê¸ˆìœµ ì§€ì‹ ë² ì´ìŠ¤ì— {len(financial_documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    
    def search_relevant_documents(self, query: str, k: int = 5) -> List[Document]:
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.vectorstore:
            return []
        
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_context_for_query(self, query: str) -> str:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±"""
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.search_relevant_documents(query, k=3)
        
        # ì‹¤ì‹œê°„ ì£¼ì‹ ë°ì´í„° ì¶”ê°€
        context_parts = []
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
        if relevant_docs:
            context_parts.append("ê´€ë ¨ ê¸ˆìœµ ì§€ì‹:")
            for doc in relevant_docs:
                context_parts.append(f"- {doc.page_content[:500]}...")
        
        # ì¿¼ë¦¬ì—ì„œ ì£¼ì‹ ì‹¬ë³¼ ì¶”ì¶œ ì‹œë„
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in ["ì‚¼ì„±ì „ì", "samsung", "005930"]):
            stock_data = self.get_financial_data("005930.KS")
            if "error" not in stock_data:
                context_parts.append(f"\nì‚¼ì„±ì „ì ì‹¤ì‹œê°„ ì •ë³´:\n{json.dumps(stock_data, ensure_ascii=False, indent=2)}")
        
        return "\n".join(context_parts) if context_parts else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
rag_service = FinancialRAGService()
