"""
ë‰´ìŠ¤ ì—ì´ì „íŠ¸
ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘, ë¶„ì„, ìš”ì•½ ì „ë¬¸ ì—ì´ì „íŠ¸
"""

from typing import Dict, Any, List, Optional
from .base_agent import BaseAgent
from app.services.workflow_components import news_service


class NewsAgent(BaseAgent):
    """ðŸ“° ë‰´ìŠ¤ ì—ì´ì „íŠ¸ - ê¸ˆìœµ ë‰´ìŠ¤ ì „ë¬¸ê°€"""
    
    def __init__(self):
        super().__init__(purpose="news")
        self.agent_name = "news_agent"
    
    def get_prompt_template(self) -> str:
        """ë‰´ìŠ¤ ë¶„ì„ ì „ëžµ ê²°ì • í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return """ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì‚¬ìš©ìž ìš”ì²­ì— ë”°ë¼ ìµœì ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì „ëžµì„ ê²°ì •í•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ìž ìš”ì²­
"{user_query}"

## ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼
- ì£¼ìš” ì˜ë„: {primary_intent}
- ë³µìž¡ë„: {complexity_level}
- í•„ìš” ì„œë¹„ìŠ¤: {required_services}

## ë‰´ìŠ¤ ìˆ˜ì§‘ ì „ëžµ ê²°ì •
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

search_strategy: [ê²€ìƒ‰ ì „ëžµ - specific/general/market/sector/company ì¤‘ í•˜ë‚˜]
search_query: [ì‹¤ì œ ê²€ìƒ‰ì— ì‚¬ìš©í•  ì¿¼ë¦¬ - ì˜ì–´ë¡œ ìž‘ì„± (ì˜ˆ: Samsung Electronics, KOSPI, SK Hynix)]
news_sources: [ë‰´ìŠ¤ ì†ŒìŠ¤ - google/mk/both]
time_range: [ì‹œê°„ ë²”ìœ„ - today/week/month]
analysis_depth: [ë¶„ì„ ê¹Šì´ - summary/detailed/comprehensive]
focus_areas: [ì§‘ì¤‘ ì˜ì—­ - price_impact/fundamental/technical/sentiment]

## ì¤‘ìš”: search_queryëŠ” ë°˜ë“œì‹œ ì˜ì–´ë¡œ ìž‘ì„±í•˜ì„¸ìš”!
- í•œêµ­ ê¸°ì—…ëª…ì€ ì˜ì–´ë¡œ ë³€í™˜ (ì˜ˆ: ì‚¼ì„±ì „ìž â†’ Samsung Electronics, SKí•˜ì´ë‹‰ìŠ¤ â†’ SK Hynix, í˜„ëŒ€ìžë™ì°¨ â†’ Hyundai Motor)
- ì‹œìž¥/ì§€ìˆ˜ëª…ë„ ì˜ì–´ë¡œ (ì˜ˆ: ì½”ìŠ¤í”¼ â†’ KOSPI, ë°˜ë„ì²´ â†’ Semiconductor)

## ì „ëžµ ì˜ˆì‹œ

ìš”ì²­: "ì‚¼ì„±ì „ìž ë‰´ìŠ¤ ì•Œë ¤ì¤˜"
search_strategy: company
search_query: Samsung Electronics
news_sources: both
time_range: today
analysis_depth: detailed
focus_areas: price_impact,fundamental

ìš”ì²­: "ì˜¤ëŠ˜ ì‹œìž¥ ë‰´ìŠ¤"
search_strategy: market
search_query: ì˜¤ëŠ˜ ì£¼ì‹ì‹œìž¥ ë™í–¥
news_sources: google
time_range: today
analysis_depth: comprehensive
focus_areas: sentiment,price_impact

ìš”ì²­: "ë°˜ë„ì²´ ì—…ì¢… ë‰´ìŠ¤"
search_strategy: sector
search_query: ë°˜ë„ì²´ ì—…ì¢… ë‰´ìŠ¤
news_sources: both
time_range: week
analysis_depth: detailed
focus_areas: fundamental,technical

## ì‘ë‹µ í˜•ì‹
search_strategy: [ê°’]
search_query: [ê°’]
news_sources: [ê°’]
time_range: [ê°’]
analysis_depth: [ê°’]
focus_areas: [ê°’]"""
    
    def parse_news_strategy(self, response_text: str) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ì „ëžµ íŒŒì‹±"""
        try:
            lines = response_text.strip().split('\n')
            result = {}
            
            for line in lines:
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    if key == 'search_strategy':
                        result['search_strategy'] = value
                    elif key == 'search_query':
                        result['search_query'] = value
                    elif key == 'news_sources':
                        result['news_sources'] = value
                    elif key == 'time_range':
                        result['time_range'] = value
                    elif key == 'analysis_depth':
                        result['analysis_depth'] = value
                    elif key == 'focus_areas':
                        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì˜ì—­ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        areas = [a.strip() for a in value.split(',') if a.strip()]
                        result['focus_areas'] = areas
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            result.setdefault('search_strategy', 'general')
            result.setdefault('search_query', 'ê¸ˆìœµ ë‰´ìŠ¤')
            result.setdefault('news_sources', 'both')
            result.setdefault('time_range', 'today')
            result.setdefault('analysis_depth', 'detailed')
            result.setdefault('focus_areas', ['price_impact'])
            
            return result
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ì „ëžµ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {
                'search_strategy': 'general',
                'search_query': 'ê¸ˆìœµ ë‰´ìŠ¤',
                'news_sources': 'both',
                'time_range': 'today',
                'analysis_depth': 'detailed',
                'focus_areas': ['price_impact']
            }
    
    def generate_news_analysis_prompt(self, news_data: List[Dict[str, Any]], strategy: Dict[str, Any], user_query: str) -> str:
        """ë‰´ìŠ¤ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë‰´ìŠ¤ ì• ë„ë¦¬ìŠ¤íŠ¸ìž…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìžì—ê²Œ ìµœì ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ìž ìš”ì²­
"{user_query}"

## ìˆ˜ì§‘ ì „ëžµ
- ê²€ìƒ‰ ì „ëžµ: {strategy.get('search_strategy', 'general')}
- ë¶„ì„ ê¹Šì´: {strategy.get('analysis_depth', 'detailed')}
- ì§‘ì¤‘ ì˜ì—­: {', '.join(strategy.get('focus_areas', ['price_impact']))}

## ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ({len(news_data)}ê±´)
{self._format_news_data(news_data)}

## ë¶„ì„ ìš”ì²­ì‚¬í•­

### 1. ðŸ“° ë‰´ìŠ¤ ìš”ì•½ ë° í•µì‹¬ í¬ì¸íŠ¸
- ê°€ìž¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ 3ê±´ ì„ ë³„
- ê° ë‰´ìŠ¤ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½ (2-3ì¤„)
- **ë°˜ë“œì‹œ ê° ë‰´ìŠ¤ì˜ ì •í™•í•œ ì¶œì²˜(source)ì™€ ë°œí–‰ì¼(published)ì„ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”**
- ì‹œìž¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ í‰ê°€ (High/Medium/Low)
- ì¶œì²˜ í‘œê¸° ì˜ˆì‹œ: (ì¶œì²˜: Reuters, 2024-05-15)

### 2. ðŸ“ˆ ì‹œìž¥ ì˜í–¥ ë¶„ì„
- ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ ì˜ˆìƒ (ìƒìŠ¹/í•˜ë½/ì¤‘ë¦½)
- ì˜í–¥ ì •ë„ ë° ê·¼ê±° ì„¤ëª…
- ë‹¨ê¸°/ì¤‘ê¸°/ìž¥ê¸° ê´€ì ì—ì„œì˜ ë¶„ì„

### 3. ðŸ’¡ íˆ¬ìžìž ê´€ì 
- íˆ¬ìžìžë“¤ì´ ì£¼ëª©í•´ì•¼ í•  í¬ì¸íŠ¸
- ë¦¬ìŠ¤í¬ ìš”ì†Œ ë° ê¸°íšŒ ìš”ì†Œ
- ì¶”ì²œ í–‰ë™ ë°©í–¥ (ê´€ì°°/ë§¤ìˆ˜/ë§¤ë„/ë³´ìœ )

### 4. ðŸ” ì¶”ê°€ ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸
- ì§€ì†ì ìœ¼ë¡œ ê´€ì°°í•´ì•¼ í•  ì§€í‘œë‚˜ ì´ë²¤íŠ¸
- ê´€ë ¨ ì¢…ëª©ì´ë‚˜ ì—…ì¢… ì˜í–¥
- í–¥í›„ ì „ë§ ë° ì‹œë‚˜ë¦¬ì˜¤

## ì‘ë‹µ í˜•ì‹
ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í†¤ìœ¼ë¡œ ìž‘ì„±í•˜ë˜, ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ê° ì„¹ì…˜ë³„ë¡œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”.

## ì£¼ì˜ì‚¬í•­
- ê°ê´€ì ì´ê³  ê· í˜• ìž¡ížŒ ë¶„ì„ ì œê³µ
- ê³¼ë„í•œ íˆ¬ìž ê¶Œìœ  ì§€ì–‘
- ê°œì¸ íˆ¬ìžìžì˜ ìƒí™©ì€ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŒì„ ëª…ì‹œ"""
    
    def _format_news_data(self, news_data: List[Dict[str, Any]]) -> str:
        """ë‰´ìŠ¤ ë°ì´í„° í¬ë§·íŒ…"""
        if not news_data:
            return "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for i, news in enumerate(news_data[:10], 1):  # ìµœëŒ€ 10ê±´ë§Œ í‘œì‹œ
            title = news.get('title', 'ì œëª© ì—†ìŒ')
            summary = news.get('summary', 'ìš”ì•½ ì—†ìŒ')
            source = news.get('source', 'ì¶œì²˜ ë¶ˆëª…')
            published = news.get('published', 'ë‚ ì§œ ë¶ˆëª…')
            
            formatted.append(f"""
**{i}. {title}**
- ì¶œì²˜: {source} | ë‚ ì§œ: {published}
- ìš”ì•½: {summary}
---""")
        
        return "\n".join(formatted)
    
    async def process(self, user_query: str, query_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì²˜ë¦¬ (async)"""
        try:
            self.log(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìž‘: {user_query}")
            
            # LLMì´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì „ëžµ ê²°ì •
            prompt = self.get_prompt_template().format(
                user_query=user_query,
                primary_intent=query_analysis.get('primary_intent', 'news'),
                complexity_level=query_analysis.get('complexity_level', 'simple'),
                required_services=query_analysis.get('required_services', [])
            )
            
            response = self.llm.invoke(prompt)
            strategy = self.parse_news_strategy(response.content.strip())
            
            print(f"ðŸ” [NewsAgent] ìƒì„±ëœ ì „ëžµ:")
            print(f"   - search_strategy: {strategy.get('search_strategy')}")
            print(f"   - search_query: {strategy.get('search_query')}")
            print(f"   - news_sources: {strategy.get('news_sources')}")
            
            # ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ (async)
            news_data = []
            mk_context = ""  # ë§¤ì¼ê²½ì œ ì»¨í…ìŠ¤íŠ¸ëŠ” ë³„ë„ë¡œ ì €ìž¥
            
            try:
                if strategy['news_sources'] in ['google', 'both']:
                    print(f"ðŸ“° [NewsAgent] Google RSSì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìž‘: {strategy['search_query']}")
                    # async í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ - ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                    google_news = await news_service.get_comprehensive_news(
                        query=strategy['search_query']
                    )
                    
                    print(f"   âœ… [NewsAgent] Google RSS ê²°ê³¼: {len(google_news) if google_news else 0}ê°œ")
                    
                    if google_news and isinstance(google_news, list):
                        news_data.extend(google_news)
                
                if strategy['news_sources'] in ['mk', 'both']:
                    # ë§¤ì¼ê²½ì œ KG ì»¨í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ í•µì‹¬ í‚¤ì›Œë“œ ì‚¬ìš©
                    # ì˜ˆ: "ê¸ˆë¦¬ ë‰´ìŠ¤ ë¶„ì„í•´ì¤˜" â†’ "ê¸ˆë¦¬"
                    korean_keyword = self._extract_korean_keyword(user_query)
                    print(f"   ðŸ“š [NewsAgent] ë§¤ì¼ê²½ì œ KG ê²€ìƒ‰ í‚¤ì›Œë“œ: {korean_keyword}")
                    
                    # async í•¨ìˆ˜ í˜¸ì¶œ - ë¬¸ìžì—´ ë°˜í™˜
                    mk_context = await news_service.get_analysis_context_from_kg(
                        query=korean_keyword,
                        limit=5
                    )
                
                # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                news_data = self._deduplicate_news(news_data)
                
            except Exception as e:
                self.log(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                news_data = []
                mk_context = ""
            
            # ë‰´ìŠ¤ ë¶„ì„
            if news_data or mk_context:
                analysis_prompt = self.generate_news_analysis_prompt(news_data, strategy, user_query)
                
                # ë§¤ì¼ê²½ì œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                if mk_context:
                    analysis_prompt += f"\n\n{mk_context}"
                
                analysis_response = self.llm.invoke(analysis_prompt)
                analysis_result = analysis_response.content
                
                self.log(f"ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ: {len(news_data or [])}ê±´")
            else:
                analysis_result = "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
                self.log("ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            return {
                'success': True,
                'news_data': news_data,
                'analysis_result': analysis_result,
                'strategy': strategy,
                'mk_context': mk_context
            }
            
        except Exception as e:
            self.log(f"ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                'news_data': [],
                'analysis_result': "ë‰´ìŠ¤ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            }
    
    def _extract_korean_keyword(self, user_query: str) -> str:
        """í•œêµ­ì–´ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        ì˜ˆ: "ì‚¼ì„±ì „ìž ë‰´ìŠ¤ ì•Œë ¤ì¤˜" â†’ "ì‚¼ì„±ì „ìž"
            "ê¸ˆë¦¬ ë‰´ìŠ¤ ë¶„ì„í•´ì¤˜" â†’ "ê¸ˆë¦¬"
        """
        # ì œê±°í•  ë¶ˆìš©ì–´
        stopwords = ['ë‰´ìŠ¤', 'ì•Œë ¤ì¤˜', 'ë¶„ì„', 'í•´ì¤˜', 'ê´€ë ¨', 'ìµœì‹ ', 'ì˜¤ëŠ˜', 'ì–´ì œ']
        
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        words = user_query.split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        keywords = [w for w in words if w not in stopwords]
        
        # í‚¤ì›Œë“œê°€ ìžˆìœ¼ë©´ ì²« ë²ˆì§¸, ì—†ìœ¼ë©´ ì›ë³¸
        return keywords[0] if keywords else user_query
    
    def _deduplicate_news(self, news_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        unique_news = []
        
        for news in news_data:
            title = news.get('title', '').strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        unique_news.sort(key=lambda x: x.get('published', ''), reverse=True)
        return unique_news

