# ê°„ë‹¨í•œ Pinecone RAG ì„œë¹„ìŠ¤ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from pinecone import Pinecone
from app.services.pinecone_config import (
    PINECONE_API_KEY, PINECONE_INDEX_NAME, EMBEDDING_MODEL_NAME, 
    MAX_LENGTH, TOP_K, DEFAULT_NAMESPACE
)

# ì „ì—­ ë³€ìˆ˜
_pinecone_client = None
_pinecone_index = None
_tokenizer = None
_model = None
_device = None


def get_pinecone_client():
    """Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global _pinecone_client
    if _pinecone_client is None:
        _pinecone_client = Pinecone(api_key=PINECONE_API_KEY)
        print("âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    return _pinecone_client


def get_pinecone_index():
    """Pinecone ì¸ë±ìŠ¤ ì—°ê²°"""
    global _pinecone_index
    if _pinecone_index is None:
        client = get_pinecone_client()
        _pinecone_index = client.Index(PINECONE_INDEX_NAME)
        print(f"âœ… Pinecone ì¸ë±ìŠ¤ ì—°ê²°: {PINECONE_INDEX_NAME}")
    return _pinecone_index


def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
    global _tokenizer, _model, _device
    if _tokenizer is None or _model is None:
        print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {EMBEDDING_MODEL_NAME}")
        _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        _tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
        _model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME).to(_device).eval()
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return _tokenizer, _model, _device


def mean_pooling(last_hidden_state, attention_mask):
    """í‰ê·  í’€ë§ìœ¼ë¡œ ë¬¸ì¥ ì„ë² ë”© ìƒì„±"""
    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    summed = (last_hidden_state * mask).sum(dim=1)
    counts = mask.sum(dim=1).clamp(min=1e-9)
    return summed / counts


def embed_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    tokenizer, model, device = get_embedding_model()
    
    with torch.no_grad():
        encoded = tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        ).to(device)

        outputs = model(**encoded)
        last_hidden_state = outputs.last_hidden_state

        embeddings = mean_pooling(last_hidden_state, encoded["attention_mask"])
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

        return embeddings.cpu().numpy()[0]


async def search_pinecone(query: str, top_k: int = None, namespace: str = None):
    """Pineconeì—ì„œ ê²€ìƒ‰"""
    if top_k is None:
        top_k = TOP_K
    if namespace is None:
        namespace = DEFAULT_NAMESPACE
        
    try:
        index = get_pinecone_index()
        query_embedding = embed_text(query)
        
        # ë¹„ë™ê¸°ë¡œ Pinecone ì¿¼ë¦¬ ì‹¤í–‰
        import asyncio
        results = await asyncio.to_thread(
            index.query,
            vector=query_embedding.tolist(),
            top_k=top_k,
            include_metadata=True,
            namespace=namespace
        )
        
        return results
    except Exception as e:
        print(f"âŒ Pinecone ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None


async def get_context_for_query(query: str, top_k: int = 5, namespace: str = None):
    """ì¿¼ë¦¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ (namespace ì§€ì›)"""
    try:
        results = await search_pinecone(query, top_k=top_k, namespace=namespace)
        
        # resultsê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
        if results is None:
            print("âš ï¸ Pinecone ê²€ìƒ‰ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
            return ""
        
        if results and hasattr(results, 'matches') and results.matches:
            context_parts = []
            for match in results.matches:
                text = match.metadata.get("text", "") if hasattr(match, 'metadata') else ""
                if text:
                    context_parts.append(text)
            
            if context_parts:
                context = "\n".join(context_parts)
                print(f"âœ… Pineconeì—ì„œ {len(context_parts)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (namespace: {namespace or 'default'})")
                return context
            else:
                print("â„¹ï¸ Pineconeì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (í…ìŠ¤íŠ¸ ì—†ìŒ)")
                return ""
        else:
            print("â„¹ï¸ Pineconeì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return ""
            
    except Exception as e:
        print(f"âŒ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return ""


class PineconeRAGService:
    """Pinecone ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)"""
    
    def __init__(self):
        self.initialized = False
    
    def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # Pinecone ì´ˆê¸°í™”
            get_pinecone_client()
            get_pinecone_index()
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            get_embedding_model()
            
            self.initialized = True
            print("âœ… Pinecone RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ Pinecone RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def get_context_for_query(self, query: str, top_k: int = 5) -> str:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)"""
        try:
            results = await search_pinecone(query, top_k=top_k)
            
            # resultsê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
            if results is None:
                print("âš ï¸ Pinecone ê²€ìƒ‰ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
                return ""
            
            if results and hasattr(results, 'matches') and results.matches:
                context_parts = []
                for match in results.matches:
                    text = match.metadata.get("text", "") if hasattr(match, 'metadata') else ""
                    if text:
                        context_parts.append(text)
                
                if context_parts:
                    context = "\n".join(context_parts)
                    print(f"âœ… Pineconeì—ì„œ {len(context_parts)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
                    return context
                else:
                    print("â„¹ï¸ Pineconeì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (í…ìŠ¤íŠ¸ ì—†ìŒ)")
                    return ""
            else:
                print("â„¹ï¸ Pineconeì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return ""
                
        except Exception as e:
            print(f"âŒ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    async def search(self, query: str, top_k: int = 5) -> list:
        """ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜"""
        try:
            results = await search_pinecone(query, top_k=top_k)
            
            if results and results.matches:
                formatted_results = []
                for match in results.matches:
                    formatted_results.append({
                        "id": match.id,
                        "score": match.score,
                        "text": match.metadata.get("text", ""),
                        "metadata": match.metadata
                    })
                return formatted_results
            else:
                return []
                
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_stats(self):
        """ì¸ë±ìŠ¤ í†µê³„ ë°˜í™˜"""
        try:
            index = get_pinecone_index()
            stats = index.describe_index_stats()
            return {
                "total_vector_count": stats.total_vector_count,
                "dimension": stats.dimension,
                "metric": stats.metric
            }
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None


# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)
pinecone_rag_service = PineconeRAGService()
