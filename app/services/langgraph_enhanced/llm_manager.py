"""
LLM ê´€ë¦¬ì (Gemini 2.0 Flash ì „ìš©)
ê¹”ë”í•˜ê²Œ Geminië§Œ ì‚¬ìš©í•˜ë„ë¡ ë‹¨ìˆœí™”
"""

from typing import Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from app.config import settings


class LLMManager:
    """LLM ê´€ë¦¬ì (Gemini ì „ìš©)"""
    
    def __init__(self):
        self.llm_cache = {}
        self.default_model = "gemini-2.0-flash"  # ì •ì‹ 2.0 ë²„ì „, ë†’ì€ í• ë‹¹ëŸ‰
    
    def get_llm(self, 
                model_name: Optional[str] = None, 
                temperature: float = 0.7,
                purpose: str = "general",
                **kwargs) -> ChatGoogleGenerativeAI:
        """
        Gemini LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìš©ë„ë³„ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
        """
        # ëª¨ë¸ëª…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        if model_name is None:
            model_name = self.default_model
        
        # ìš©ë„ë³„ ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì„¤ì •
        optimized_params = self._get_optimized_params(purpose, temperature, **kwargs)
        
        # ìºì‹œì—ì„œ í™•ì¸
        cache_key = f"{model_name}_{purpose}_{optimized_params['temperature']}_{hash(str(optimized_params))}"
        if cache_key in self.llm_cache:
            return self.llm_cache[cache_key]
        
        # API í‚¤ í™•ì¸
        google_api_key = settings.google_api_key
        if not google_api_key:
            raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Gemini LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=google_api_key,
            **optimized_params
        )
        
        # ìºì‹œì— ì €ì¥
        self.llm_cache[cache_key] = llm
        
        print(f"ğŸ¤– Gemini LLM ë¡œë“œ: {model_name} ({purpose}, temperature: {optimized_params['temperature']})")
        return llm
    
    def _get_optimized_params(self, purpose: str, temperature: float, **kwargs) -> dict:
        """ìš©ë„ë³„ ìµœì í™”ëœ LLM íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        base_params = {
            "temperature": temperature,
            "top_p": 0.9,
            "top_k": 40,
            "max_output_tokens": 2048,
            **kwargs
        }
        
        # ìš©ë„ë³„ ìµœì í™”
        if purpose == "classification":
            return {
                **base_params,
                "temperature": 0.1,  # ë¶„ë¥˜ëŠ” ì¼ê´€ì„± ì¤‘ìš”
                "max_output_tokens": 512,  # êµ¬ì¡°í™”ëœ ì‘ë‹µì„ ìœ„í•´ ì¶©ë¶„í•œ í† í°
                "top_p": 0.8
            }
        elif purpose == "analysis":
            return {
                **base_params,
                "temperature": 0.3,  # ë¶„ì„ì€ ê· í˜•ì¡íŒ ì°½ì˜ì„±
                "max_output_tokens": 4096,  # ìƒì„¸í•œ ë¶„ì„ í•„ìš”
                "top_p": 0.9
            }
        elif purpose == "news":
            return {
                **base_params,
                "temperature": 0.2,  # ë‰´ìŠ¤ëŠ” ê°ê´€ì„± ì¤‘ìš”
                "max_output_tokens": 2048,
                "top_p": 0.85
            }
        elif purpose == "knowledge":
            return {
                **base_params,
                "temperature": 0.4,  # êµìœ¡ì  ì„¤ëª…ì€ ì ë‹¹í•œ ì°½ì˜ì„±
                "max_output_tokens": 3072,
                "top_p": 0.9
            }
        elif purpose == "response":
            return {
                **base_params,
                "temperature": 0.7,  # ì‘ë‹µì€ ìì—°ìŠ¤ëŸ¬ì›€ ì¤‘ìš”
                "max_output_tokens": 2048,
                "top_p": 0.9
            }
        else:  # general
            return base_params
    
    def get_default_llm(self, temperature: float = 0.7, purpose: str = "general", **kwargs) -> ChatGoogleGenerativeAI:
        """ê¸°ë³¸ Gemini LLM ë°˜í™˜"""
        return self.get_llm(model_name=None, temperature=temperature, purpose=purpose, **kwargs)
    
    def clear_cache(self):
        """LLM ìºì‹œ ì´ˆê¸°í™”"""
        self.llm_cache.clear()
        print("ğŸ§¹ LLM ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì „ì—­ LLM ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
llm_manager = LLMManager()


def get_gemini_llm(model_name: Optional[str] = None, 
                   temperature: float = 0.7, 
                   **kwargs) -> ChatGoogleGenerativeAI:
    """Gemini LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (í¸ì˜ í•¨ìˆ˜)"""
    return llm_manager.get_llm(model_name, temperature, **kwargs)


def get_default_gemini_llm(temperature: float = 0.7, **kwargs) -> ChatGoogleGenerativeAI:
    """ê¸°ë³¸ Gemini LLM ë°˜í™˜ (í¸ì˜ í•¨ìˆ˜)"""
    return llm_manager.get_default_llm(temperature, **kwargs)
